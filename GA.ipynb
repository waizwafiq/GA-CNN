{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waizwafiq/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_classes, kernel_sizes=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        if kernel_sizes is None:\n",
    "            k = [[3]]\n",
    "        else:\n",
    "            k = kernel_sizes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_channels[0], kernel_size=k[0][0], stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()  # Add module list for pooling layers\n",
    "\n",
    "        for i in range(1, len(hidden_channels)):\n",
    "            self.conv_layers.append(nn.Conv2d(hidden_channels[i-1], hidden_channels[i], kernel_size=k[i][0], stride=1, padding=1))\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool_layers.append(nn.MaxPool2d(kernel_size=k[i][1], stride=2))  # Add max pooling layer\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        for conv_layer, pool_layer in zip(self.conv_layers, self.pool_layers):  # Iterate over conv and pool layers\n",
    "            out = conv_layer(out)\n",
    "            out = self.relu(out)\n",
    "            out = pool_layer(out)  # Apply max pooling\n",
    "\n",
    "        out = F.avg_pool2d(out, kernel_size=out.size()[2:])  # Global average pooling\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        if not self.training:\n",
    "            out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, data_loader, epochSize=20):\n",
    "    train_loader, test_loader = data_loader[0], data_loader[1]\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    loss_per_epoch = []\n",
    "    train_acc_per_epoch = []\n",
    "    test_acc_per_epoch = []\n",
    "    total_acc_per_epoch = []\n",
    "    time_per_epoch = []\n",
    "    exec_time = []\n",
    "\n",
    "    start_total_time = time.time()\n",
    "    for epoch in range(epochSize):\n",
    "\n",
    "        loss = 0\n",
    "        start_epoch_time = time.time()\n",
    "\n",
    "        count = 1\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s {(count/count_batch_train)*100:.3f}%\", end='')\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predict_batch = model(input_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_batch = loss_fn(predict_batch, target_batch)\n",
    "\n",
    "            # Backward pass and update weights\n",
    "            loss_batch.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss += loss_batch.item()  # store the loss\n",
    "            count += 1\n",
    "            print('\\r', end='', flush=True)\n",
    "\n",
    "        loss_per_epoch.append(loss)\n",
    "        # print(loss)\n",
    "\n",
    "        # CALCULATE TRAIN ACCURACY\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            count = 1\n",
    "            for images, labels in train_loader:\n",
    "                print(f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s || Calculating Training Accuracy... {(count/count_batch_train)*100:.3f}%\", end='', flush=True)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get the predicted labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Update counts\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "                print('\\r', end='', flush=True)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        train_accuracy = correct / total\n",
    "        train_acc_per_epoch.append(train_accuracy)\n",
    "\n",
    "        # CALCULATE TEST ACCURACY\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            count = 1\n",
    "            for images, labels in test_loader:\n",
    "                print(f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s || Calculating Testing Accuracy... {(count/count_batch_test)*100:.3f}%\", end='', flush=True)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get the predicted labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Update counts\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "                print('\\r', end='', flush=True)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        test_accuracy = correct / total\n",
    "        test_acc_per_epoch.append(test_accuracy)\n",
    "\n",
    "        time_epoch = time.time() - start_epoch_time\n",
    "        time_current = time.time() - start_total_time\n",
    "\n",
    "        time_per_epoch.append(time_epoch)\n",
    "        exec_time.append(time_current)\n",
    "\n",
    "        total_accuracy = 0.3*train_accuracy + 0.7*test_accuracy\n",
    "        total_acc_per_epoch.append(total_accuracy)\n",
    "\n",
    "        print(f'Epoch: {epoch+1} || Loss: {loss} || Train Acc: {train_accuracy * 100:.4f}% || Test Acc: {test_accuracy * 100:.4f}% || Total Acc: {total_accuracy * 100:.4f}% || Epoch Time: {time_epoch:.4f} s || Current Runtime: {time_current:.4f} s')\n",
    "\n",
    "    output = {\n",
    "        'loss': loss_per_epoch,\n",
    "        'train_acc': train_acc_per_epoch,\n",
    "        'test_acc': test_acc_per_epoch,\n",
    "        'total_acc': total_acc_per_epoch,\n",
    "        'epoch_time': time_per_epoch,\n",
    "        'exec_time': exec_time\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, dataloader):\n",
    "    # Create ConvNet instance with the provided individual configuration\n",
    "    h = individual[0]\n",
    "    k_0 = [[2]]\n",
    "    k_n = k_0 + individual[1]\n",
    "\n",
    "    print(individual)\n",
    "    model = ConvNet(1, hidden_channels=h, num_classes=5, kernel_sizes=k_n)\n",
    "    result = trainModel(model, dataloader, epochSize=10)\n",
    "    return result['total_acc'][-1]\n",
    "\n",
    "def define_fitness(population, dataloader):\n",
    "    fitness_per_individual = []\n",
    "    for individual in population:\n",
    "        fitness_per_individual.append(fitness(individual, dataloader))\n",
    "\n",
    "    return fitness_per_individual\n",
    "\n",
    "def generate_population(population_size, layers, hidden_channels_range, kernel_size_range):\n",
    "    np.random.seed(4)\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        # For each population,\n",
    "        chromosome_h = []\n",
    "        for _ in range(layers):\n",
    "            # create chromosome for the number of hidden channels per layer e.g: (layers = 3) ->[256, 128, 64]\n",
    "            filters = np.random.randint(\n",
    "                hidden_channels_range[0], hidden_channels_range[1]+1\n",
    "            )\n",
    "            chromosome_h.append(filters)\n",
    "\n",
    "        chromosome_k = []\n",
    "        for _ in range(layers):\n",
    "            # create chromosome for the kernel sizes per layer (2D list, each row represents layer)\n",
    "            kernel_size = np.random.randint(\n",
    "                kernel_size_range[0], kernel_size_range[1]+1, (1, 2)\n",
    "            )\n",
    "            chromosome_k.append(list(kernel_size[0]))\n",
    "        population.append((chromosome_h, chromosome_k))\n",
    "\n",
    "    return population\n",
    "\n",
    "def DataFrame_Pop(pop_unstructured):\n",
    "    population = []\n",
    "    for i in range(len(pop_unstructured)):\n",
    "        lst1 = np.array(pop_unstructured[i][0])\n",
    "        lst2 = np.array(pop_unstructured[i][1])\n",
    "        flattened = np.concatenate(([i+1], lst1, lst2.flatten()))\n",
    "\n",
    "        population.append(flattened.tolist())\n",
    "    col_name1 = [f\"conv_h (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name2 = [f\"conv_k (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name3 = [f\"pool_k (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name4 = list(itertools.chain(*zip(col_name2, col_name3)))\n",
    "\n",
    "    cols = ['Individual'] + col_name1 + col_name4\n",
    "    return pd.DataFrame(population, columns=cols, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 413 batches in train_loader\n",
      "There are 104 batches in test_loader\n",
      "torch.Size([16, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "data_dir = './processed_dataset'\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "dataset = datasets.DatasetFolder(data_dir, loader=torch.load, extensions=\".pt\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Set DataLoader\n",
    "batchSize = 16  # Rule of thumb is to set to the power of 2. In this case 2^7\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False) # no need to shuffle test data\n",
    "\n",
    "count_batch_train, count_batch_test = 0, 0\n",
    "for xb, yb in train_loader:\n",
    "  print(count_batch_train, end='', flush=True)\n",
    "  count_batch_train += 1\n",
    "  print(\"\\r\", end='', flush=True)\n",
    "print(f'There are {count_batch_train} batches in train_loader')\n",
    "\n",
    "for xb, yb in test_loader:\n",
    "  print(count_batch_test, end='', flush=True)\n",
    "  count_batch_test += 1\n",
    "  print(\"\\r\", end='', flush=True)\n",
    "print(f'There are {count_batch_test} batches in test_loader')\n",
    "\n",
    "for i, j in train_loader:\n",
    "    size = i.shape\n",
    "    break\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Individual</th>\n",
       "      <th>conv_h (Layer 1)</th>\n",
       "      <th>conv_k (Layer 1)</th>\n",
       "      <th>pool_k (Layer 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Individual  conv_h (Layer 1)  conv_k (Layer 1)  pool_k (Layer 1)\n",
       "0           1                 3                 1                 2\n",
       "1           2                 2                 2                 1\n",
       "2           3                 4                 1                 1\n",
       "3           4                 2                 1                 1\n",
       "4           5                 4                 2                 1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = generate_population(5, 1, [1, 4], [1, 2])\n",
    "DataFrame_Pop(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3], [[1, 2]])\n",
      "Epoch: 1 || Loss: 669.7146828174591 || Train Acc: 21.3485% || Test Acc: 20.3636% || Total Acc: 20.6591% || Epoch Time: 56.2944 s || Current Runtime: 56.2944 s\n",
      "Epoch: 2 || Loss: 647.6352045536041 || Train Acc: 27.3939% || Test Acc: 27.5758% || Total Acc: 27.5212% || Epoch Time: 50.6324 s || Current Runtime: 106.9271 s\n",
      "Epoch: 3 || Loss: 622.2057185173035 || Train Acc: 37.8333% || Test Acc: 39.6970% || Total Acc: 39.1379% || Epoch Time: 68.2943 s || Current Runtime: 175.2216 s\n",
      "Epoch: 4 || Loss: 591.2863721847534 || Train Acc: 37.7424% || Test Acc: 39.5758% || Total Acc: 39.0258% || Epoch Time: 61.6021 s || Current Runtime: 236.8241 s\n",
      "Epoch: 5 || Loss: 559.4222121238708 || Train Acc: 42.8333% || Test Acc: 43.6364% || Total Acc: 43.3955% || Epoch Time: 63.4569 s || Current Runtime: 300.2813 s\n",
      "Epoch: 6 || Loss: 532.1628519296646 || Train Acc: 43.6667% || Test Acc: 45.3939% || Total Acc: 44.8758% || Epoch Time: 64.1225 s || Current Runtime: 364.4041 s\n",
      "Epoch: 7 || Loss: 510.5627775192261 || Train Acc: 43.3939% || Test Acc: 43.6970% || Total Acc: 43.6061% || Epoch Time: 51.8923 s || Current Runtime: 416.2978 s\n",
      "Epoch: 8 || Loss: 494.150117456913 || Train Acc: 44.1818% || Test Acc: 45.6970% || Total Acc: 45.2424% || Epoch Time: 48.6157 s || Current Runtime: 464.9137 s\n",
      "Epoch: 9 || Loss: 481.3908719420433 || Train Acc: 46.5152% || Test Acc: 47.8182% || Total Acc: 47.4273% || Epoch Time: 39.9034 s || Current Runtime: 504.8173 s\n",
      "Epoch: 10 || Loss: 471.3041713833809 || Train Acc: 45.2879% || Test Acc: 46.9091% || Total Acc: 46.4227% || Epoch Time: 37.0183 s || Current Runtime: 541.8359 s\n",
      "([2], [[2, 1]])\n",
      "Epoch: 1 || Loss: 687.3320515155792 || Train Acc: 19.6364% || Test Acc: 21.4545% || Total Acc: 20.9091% || Epoch Time: 38.5900 s || Current Runtime: 38.5900 s\n",
      "Epoch: 2 || Loss: 665.5000607967377 || Train Acc: 22.9394% || Test Acc: 23.0303% || Total Acc: 23.0030% || Epoch Time: 50.4499 s || Current Runtime: 89.0402 s\n",
      "Epoch: 3 || Loss: 664.5737447738647 || Train Acc: 20.0909% || Test Acc: 19.7576% || Total Acc: 19.8576% || Epoch Time: 49.1650 s || Current Runtime: 138.2062 s\n",
      "Epoch: 4 || Loss: 664.0606126785278 || Train Acc: 29.6212% || Test Acc: 30.4848% || Total Acc: 30.2258% || Epoch Time: 48.3745 s || Current Runtime: 186.5808 s\n",
      "Epoch: 5 || Loss: 642.3983188867569 || Train Acc: 39.1212% || Test Acc: 41.0303% || Total Acc: 40.4576% || Epoch Time: 40.4003 s || Current Runtime: 226.9811 s\n",
      "Epoch: 6 || Loss: 603.4808877706528 || Train Acc: 39.9242% || Test Acc: 40.2424% || Total Acc: 40.1470% || Epoch Time: 37.7172 s || Current Runtime: 264.6985 s\n",
      "Epoch: 7 || Loss: 570.560854434967 || Train Acc: 37.0303% || Test Acc: 36.9697% || Total Acc: 36.9879% || Epoch Time: 52.7521 s || Current Runtime: 317.4510 s\n",
      "Epoch: 8 || Loss: 544.4180588722229 || Train Acc: 40.3788% || Test Acc: 41.1515% || Total Acc: 40.9197% || Epoch Time: 55.2408 s || Current Runtime: 372.6919 s\n",
      "Epoch: 9 || Loss: 524.2447981834412 || Train Acc: 42.6212% || Test Acc: 42.8485% || Total Acc: 42.7803% || Epoch Time: 45.2682 s || Current Runtime: 417.9610 s\n",
      "Epoch: 10 || Loss: 508.316423535347 || Train Acc: 41.6818% || Test Acc: 42.7879% || Total Acc: 42.4561% || Epoch Time: 32.7032 s || Current Runtime: 450.6643 s\n",
      "([4], [[1, 1]])\n",
      "Epoch: 1 || Loss: 663.2850615978241 || Train Acc: 33.1970% || Test Acc: 31.6364% || Total Acc: 32.1045% || Epoch Time: 40.2329 s || Current Runtime: 40.2329 s\n",
      "Epoch: 2 || Loss: 643.7861374616623 || Train Acc: 32.8030% || Test Acc: 29.8788% || Total Acc: 30.7561% || Epoch Time: 43.0403 s || Current Runtime: 83.2733 s\n",
      "Epoch: 3 || Loss: 619.5474767684937 || Train Acc: 41.3030% || Test Acc: 42.6061% || Total Acc: 42.2152% || Epoch Time: 45.6422 s || Current Runtime: 128.9155 s\n",
      "Epoch: 4 || Loss: 592.8078776597977 || Train Acc: 38.3182% || Test Acc: 38.5455% || Total Acc: 38.4773% || Epoch Time: 46.0939 s || Current Runtime: 175.0097 s\n",
      "Epoch: 5 || Loss: 566.8725122213364 || Train Acc: 42.9091% || Test Acc: 46.0606% || Total Acc: 45.1152% || Epoch Time: 43.7620 s || Current Runtime: 218.7718 s\n",
      "Epoch: 6 || Loss: 544.0568995475769 || Train Acc: 48.8485% || Test Acc: 49.6970% || Total Acc: 49.4424% || Epoch Time: 43.9563 s || Current Runtime: 262.7282 s\n",
      "Epoch: 7 || Loss: 525.4082406759262 || Train Acc: 43.2121% || Test Acc: 46.3030% || Total Acc: 45.3758% || Epoch Time: 33.7668 s || Current Runtime: 296.4951 s\n",
      "Epoch: 8 || Loss: 510.0045530796051 || Train Acc: 44.0758% || Test Acc: 44.8485% || Total Acc: 44.6167% || Epoch Time: 35.0480 s || Current Runtime: 331.5431 s\n",
      "Epoch: 9 || Loss: 497.62890470027924 || Train Acc: 43.5758% || Test Acc: 46.0000% || Total Acc: 45.2727% || Epoch Time: 35.2666 s || Current Runtime: 366.8098 s\n",
      "Epoch: 10 || Loss: 487.1259001493454 || Train Acc: 44.8636% || Test Acc: 46.9091% || Total Acc: 46.2955% || Epoch Time: 35.7658 s || Current Runtime: 402.5757 s\n",
      "([2], [[1, 1]])\n",
      "Epoch: 1 || Loss: 673.5335952043533 || Train Acc: 20.0758% || Test Acc: 19.6970% || Total Acc: 19.8106% || Epoch Time: 42.5082 s || Current Runtime: 42.5082 s\n",
      "Epoch: 2 || Loss: 663.7411490678787 || Train Acc: 20.9545% || Test Acc: 20.6061% || Total Acc: 20.7106% || Epoch Time: 52.3440 s || Current Runtime: 94.8522 s\n",
      "Epoch: 3 || Loss: 653.8357437849045 || Train Acc: 27.6818% || Test Acc: 26.8485% || Total Acc: 27.0985% || Epoch Time: 38.2738 s || Current Runtime: 133.1261 s\n",
      "Epoch: 4 || Loss: 634.011124253273 || Train Acc: 29.8333% || Test Acc: 29.8182% || Total Acc: 29.8227% || Epoch Time: 38.4641 s || Current Runtime: 171.5902 s\n",
      "Epoch: 5 || Loss: 610.5969961881638 || Train Acc: 29.6818% || Test Acc: 29.6970% || Total Acc: 29.6924% || Epoch Time: 39.0558 s || Current Runtime: 210.6461 s\n",
      "Epoch: 6 || Loss: 588.4810717105865 || Train Acc: 32.3333% || Test Acc: 31.9394% || Total Acc: 32.0576% || Epoch Time: 39.1173 s || Current Runtime: 249.7635 s\n",
      "Epoch: 7 || Loss: 569.1175866127014 || Train Acc: 34.8636% || Test Acc: 34.4242% || Total Acc: 34.5561% || Epoch Time: 38.5212 s || Current Runtime: 288.2849 s\n",
      "Epoch: 8 || Loss: 552.2745467424393 || Train Acc: 39.1212% || Test Acc: 38.9091% || Total Acc: 38.9727% || Epoch Time: 37.7883 s || Current Runtime: 326.0735 s\n",
      "Epoch: 9 || Loss: 537.9612572193146 || Train Acc: 42.2879% || Test Acc: 42.0606% || Total Acc: 42.1288% || Epoch Time: 37.3851 s || Current Runtime: 363.4587 s\n",
      "Epoch: 10 || Loss: 525.6228593587875 || Train Acc: 44.7424% || Test Acc: 44.1818% || Total Acc: 44.3500% || Epoch Time: 39.7152 s || Current Runtime: 403.1739 s\n",
      "([4], [[2, 1]])\n",
      "Epoch: 1 || Loss: 674.3107447624207 || Train Acc: 19.7879% || Test Acc: 20.8485% || Total Acc: 20.5303% || Epoch Time: 50.0294 s || Current Runtime: 50.0294 s\n",
      "Epoch: 2 || Loss: 664.852694272995 || Train Acc: 20.7879% || Test Acc: 21.5152% || Total Acc: 21.2970% || Epoch Time: 45.1473 s || Current Runtime: 95.1768 s\n",
      "Epoch: 3 || Loss: 654.5262486934662 || Train Acc: 32.5152% || Test Acc: 32.5455% || Total Acc: 32.5364% || Epoch Time: 46.6329 s || Current Runtime: 141.8100 s\n",
      "Epoch: 4 || Loss: 624.6453154087067 || Train Acc: 34.0909% || Test Acc: 34.4848% || Total Acc: 34.3667% || Epoch Time: 45.0372 s || Current Runtime: 186.8474 s\n",
      "Epoch: 5 || Loss: 585.6557711362839 || Train Acc: 42.0303% || Test Acc: 43.6970% || Total Acc: 43.1970% || Epoch Time: 46.4671 s || Current Runtime: 233.3147 s\n",
      "Epoch: 6 || Loss: 548.3732610940933 || Train Acc: 46.1818% || Test Acc: 47.0909% || Total Acc: 46.8182% || Epoch Time: 62.7818 s || Current Runtime: 296.0967 s\n",
      "Epoch: 7 || Loss: 520.1785233020782 || Train Acc: 44.8939% || Test Acc: 46.8485% || Total Acc: 46.2621% || Epoch Time: 63.6432 s || Current Runtime: 359.7401 s\n",
      "Epoch: 8 || Loss: 499.9094459414482 || Train Acc: 43.3182% || Test Acc: 45.5758% || Total Acc: 44.8985% || Epoch Time: 45.8074 s || Current Runtime: 405.5476 s\n",
      "Epoch: 9 || Loss: 485.1525164246559 || Train Acc: 45.3030% || Test Acc: 46.3636% || Total Acc: 46.0455% || Epoch Time: 43.6653 s || Current Runtime: 449.2130 s\n",
      "Epoch: 10 || Loss: 473.63145339488983 || Train Acc: 48.4091% || Test Acc: 49.8182% || Total Acc: 49.3955% || Epoch Time: 43.9943 s || Current Runtime: 493.2074 s\n"
     ]
    }
   ],
   "source": [
    "fitness_scores = define_fitness(pop, [train_loader, test_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48951515151515146,\n",
       " 0.48949999999999994,\n",
       " 0.5019393939393939,\n",
       " 0.46692424242424246,\n",
       " 0.4856060606060606]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wix3001_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
