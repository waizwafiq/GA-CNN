{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waizwafiq/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_classes, kernel_sizes=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        if kernel_sizes is None:\n",
    "            k = [[3]]\n",
    "        else:\n",
    "            k = kernel_sizes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_channels[0], kernel_size=k[0][0], stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()  # Add module list for pooling layers\n",
    "\n",
    "        for i in range(1, len(hidden_channels)):\n",
    "            self.conv_layers.append(nn.Conv2d(hidden_channels[i-1], hidden_channels[i], kernel_size=k[i][0], stride=1, padding=1))\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool_layers.append(nn.MaxPool2d(kernel_size=k[i][1], stride=2))  # Add max pooling layer\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        for conv_layer, pool_layer in zip(self.conv_layers, self.pool_layers):  # Iterate over conv and pool layers\n",
    "            out = conv_layer(out)\n",
    "            out = self.relu(out)\n",
    "            out = pool_layer(out)  # Apply max pooling\n",
    "\n",
    "        out = F.avg_pool2d(out, kernel_size=out.size()[2:])  # Global average pooling\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        if not self.training:\n",
    "            out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, data_loader, epochSize=20):\n",
    "    train_loader, test_loader = data_loader[0], data_loader[1]\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    loss_per_epoch = []\n",
    "    train_acc_per_epoch = []\n",
    "    test_acc_per_epoch = []\n",
    "    total_acc_per_epoch = []\n",
    "    time_per_epoch = []\n",
    "    exec_time = []\n",
    "\n",
    "    start_total_time = time.time()\n",
    "    for epoch in range(epochSize):\n",
    "\n",
    "        loss = 0\n",
    "        start_epoch_time = time.time()\n",
    "\n",
    "        count = 1\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s {(count/count_batch_train)*100:.3f}%\", end='')\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predict_batch = model(input_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_batch = loss_fn(predict_batch, target_batch)\n",
    "\n",
    "            # Backward pass and update weights\n",
    "            loss_batch.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss += loss_batch.item()  # store the loss\n",
    "            count += 1\n",
    "            print('\\r', end='', flush=True)\n",
    "\n",
    "        loss_per_epoch.append(loss)\n",
    "        # print(loss)\n",
    "\n",
    "        # CALCULATE TRAIN ACCURACY\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            count = 1\n",
    "            for images, labels in train_loader:\n",
    "                print(f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s || Calculating Training Accuracy... {(count/count_batch_train)*100:.3f}%\", end='', flush=True)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get the predicted labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Update counts\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "                print('\\r', end='', flush=True)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        train_accuracy = correct / total\n",
    "        train_acc_per_epoch.append(train_accuracy)\n",
    "\n",
    "        # CALCULATE TEST ACCURACY\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            count = 1\n",
    "            for images, labels in test_loader:\n",
    "                print(f\"Epoch: {epoch + 1} => {time.time() - start_epoch_time:.2f}s || Calculating Testing Accuracy... {(count/count_batch_test)*100:.3f}%\", end='', flush=True)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get the predicted labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Update counts\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                count += 1\n",
    "                print('\\r', end='', flush=True)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        test_accuracy = correct / total\n",
    "        test_acc_per_epoch.append(test_accuracy)\n",
    "\n",
    "        time_epoch = time.time() - start_epoch_time\n",
    "        time_current = time.time() - start_total_time\n",
    "\n",
    "        time_per_epoch.append(time_epoch)\n",
    "        exec_time.append(time_current)\n",
    "\n",
    "        total_accuracy = 0.3*train_accuracy + 0.7*test_accuracy\n",
    "        total_acc_per_epoch.append(total_accuracy)\n",
    "\n",
    "        print(f'Epoch: {epoch+1} || Loss: {loss} || Train Acc: {train_accuracy * 100:.4f}% || Test Acc: {test_accuracy * 100:.4f}% || Total Acc: {total_accuracy * 100:.4f}% || Epoch Time: {time_epoch:.4f} s || Current Runtime: {time_current:.4f} s')\n",
    "\n",
    "    output = {\n",
    "        'loss': loss_per_epoch,\n",
    "        'train_acc': train_acc_per_epoch,\n",
    "        'test_acc': test_acc_per_epoch,\n",
    "        'total_acc': total_acc_per_epoch,\n",
    "        'epoch_time': time_per_epoch,\n",
    "        'exec_time': exec_time\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, dataloader):\n",
    "    # Create ConvNet instance with the provided individual configuration\n",
    "    h = individual[0]\n",
    "    k_0 = [[2]]\n",
    "    k_n = k_0 + individual[1]\n",
    "\n",
    "    print(individual)\n",
    "    model = ConvNet(1, hidden_channels=h, num_classes=5, kernel_sizes=k_n)\n",
    "    result = trainModel(model, dataloader, epochSize=10)\n",
    "    return result['total_acc'][-1]\n",
    "\n",
    "def define_fitness(population, dataloader):\n",
    "    fitness_per_individual = []\n",
    "    for individual in population:\n",
    "        fitness_per_individual.append(fitness(individual, dataloader))\n",
    "\n",
    "    return fitness_per_individual\n",
    "\n",
    "def generate_population(population_size, layers, hidden_channels_range, kernel_size_range):\n",
    "    np.random.seed(4)\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        # For each population,\n",
    "        chromosome_h = []\n",
    "        for _ in range(layers):\n",
    "            # create chromosome for the number of hidden channels per layer e.g: (layers = 3) ->[256, 128, 64]\n",
    "            filters = np.random.randint(\n",
    "                hidden_channels_range[0], hidden_channels_range[1]+1\n",
    "            )\n",
    "            chromosome_h.append(filters)\n",
    "\n",
    "        chromosome_k = []\n",
    "        for _ in range(layers):\n",
    "            # create chromosome for the kernel sizes per layer (2D list, each row represents layer)\n",
    "            kernel_size = np.random.randint(\n",
    "                kernel_size_range[0], kernel_size_range[1]+1, (1, 2)\n",
    "            )\n",
    "            chromosome_k.append(list(kernel_size[0]))\n",
    "        population.append((chromosome_h, chromosome_k))\n",
    "\n",
    "    return population\n",
    "\n",
    "def DataFrame_Pop(pop_unstructured):\n",
    "    population = []\n",
    "    for i in range(len(pop_unstructured)):\n",
    "        lst1 = np.array(pop_unstructured[i][0])\n",
    "        lst2 = np.array(pop_unstructured[i][1])\n",
    "        flattened = np.concatenate(([i+1], lst1, lst2.flatten()))\n",
    "\n",
    "        population.append(flattened.tolist())\n",
    "    col_name1 = [f\"conv_h (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name2 = [f\"conv_k (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name3 = [f\"pool_k (Layer {i+1})\" for i in range(len(pop_unstructured[0][0]))]\n",
    "    col_name4 = list(itertools.chain(*zip(col_name2, col_name3)))\n",
    "\n",
    "    cols = ['Individual'] + col_name1 + col_name4\n",
    "    return pd.DataFrame(population, columns=cols, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 413 batches in train_loader\n",
      "There are 104 batches in test_loader\n",
      "torch.Size([16, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "data_dir = './processed_dataset'\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "dataset = datasets.DatasetFolder(data_dir, loader=torch.load, extensions=\".pt\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Set DataLoader\n",
    "batchSize = 16  # Rule of thumb is to set to the power of 2. In this case 2^7\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False) # no need to shuffle test data\n",
    "\n",
    "count_batch_train, count_batch_test = 0, 0\n",
    "for xb, yb in train_loader:\n",
    "  print(count_batch_train, end='', flush=True)\n",
    "  count_batch_train += 1\n",
    "  print(\"\\r\", end='', flush=True)\n",
    "print(f'There are {count_batch_train} batches in train_loader')\n",
    "\n",
    "for xb, yb in test_loader:\n",
    "  print(count_batch_test, end='', flush=True)\n",
    "  count_batch_test += 1\n",
    "  print(\"\\r\", end='', flush=True)\n",
    "print(f'There are {count_batch_test} batches in test_loader')\n",
    "\n",
    "for i, j in train_loader:\n",
    "    size = i.shape\n",
    "    break\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Individual</th>\n",
       "      <th>conv_h (Layer 1)</th>\n",
       "      <th>conv_k (Layer 1)</th>\n",
       "      <th>pool_k (Layer 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Individual  conv_h (Layer 1)  conv_k (Layer 1)  pool_k (Layer 1)\n",
       "0           1                14                 2                 1\n",
       "1           2                11                 1                 3\n",
       "2           3                13                 3                 5\n",
       "3           4                11                 2                 1\n",
       "4           5                 8                 3                 5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = generate_population(5, 1, [4, 16], [1, 5])\n",
    "DataFrame_Pop(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([6], [[3, 3]])\n",
      "Epoch: 1 || Loss: 663.3225330114365 || Train Acc: 40.1515% || Test Acc: 41.8182% || Total Acc: 41.3182 || Epoch Time: 45.8957 s || Current Runtime: 45.8958 s\n",
      "Epoch: 2 || Loss: 642.9703561067581 || Train Acc: 56.4848% || Test Acc: 56.9697% || Total Acc: 56.8242 || Epoch Time: 45.9603 s || Current Runtime: 91.8561 s\n",
      "Epoch: 3 => 5.17s 22.034%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m indv \u001b[39m=\u001b[39m pop[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m fitness(indv, [train_loader, test_loader])\n",
      "Cell \u001b[0;32mIn[21], line 48\u001b[0m, in \u001b[0;36mfitness\u001b[0;34m(individual, dataloader)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(individual)\n\u001b[1;32m     47\u001b[0m model \u001b[39m=\u001b[39m ConvNet(\u001b[39m1\u001b[39m, hidden_channels\u001b[39m=\u001b[39mh, num_classes\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, kernel_sizes\u001b[39m=\u001b[39mk_n)\n\u001b[0;32m---> 48\u001b[0m result \u001b[39m=\u001b[39m trainModel(model, dataloader, epochSize\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mtotal_acc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(model, data_loader, epochSize)\u001b[0m\n\u001b[1;32m     26\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m predict_batch \u001b[39m=\u001b[39m model(input_batch)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss_batch \u001b[39m=\u001b[39m loss_fn(predict_batch, target_batch)\n",
      "File \u001b[0;32m~/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     25\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(out)\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m conv_layer, pool_layer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_layers):  \u001b[39m# Iterate over conv and pool layers\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/FSKTM/Semester 6/WIX3001 Soft Computing/wix3001_assignment/wix3001_assignment/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indv = pop[0]\n",
    "fitness(indv, [train_loader, test_loader])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wix3001_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
